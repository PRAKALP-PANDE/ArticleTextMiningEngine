{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf324302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9e8135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Requirement already satisfied: syllables in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (1.0.9)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: cmudict<2.0.0,>=1.0.11 in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from syllables) (1.0.16)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=5.1 in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from syllables) (6.0.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from cmudict<2.0.0,>=1.0.11->syllables) (6.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\prakalp\\anaconda3\\lib\\site-packages (from importlib-metadata<7.0,>=5.1->syllables) (3.11.0)\n"
     ]
    }
   ],
   "source": [
    "pip install syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f31ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prakalp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prakalp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import syllables\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4ab645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14515f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_stats(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words_per_sentence = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "    avg_sentence_length = sum(words_per_sentence) / len(words_per_sentence)\n",
    "    avg_words_per_sentence = sum(words_per_sentence) / len(sentences)\n",
    "\n",
    "    return avg_sentence_length, avg_words_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a1fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complex_word_stats(text, complex_words):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    complex_word_count = sum(word in complex_words for word in words)\n",
    "    percentage_complex_words = (complex_word_count / len(words)) * 100\n",
    "\n",
    "    return percentage_complex_words, complex_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c3be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fog_index(avg_sentence_length, percentage_complex_words):\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    return fog_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f455c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_syllables_per_word(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    syllables_per_word = sum([syllables.estimate(word) for word in words]) / len(words)\n",
    "    return syllables_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b92fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text, personal_pronouns):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    personal_pronoun_count = sum(word.lower() in personal_pronouns for word in words)\n",
    "    return personal_pronoun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0c9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_word_length(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    return avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f03125bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('input.xlsx')\n",
    "urls = df['URL']\n",
    "url_ids = df['URL_ID']\n",
    "\n",
    "complex_words = ['abject', 'aberration', 'abnegation']\n",
    "\n",
    "personal_pronouns = ['I', 'me', 'my', 'you', 'your']\n",
    "\n",
    "output_data = pd.DataFrame(columns=['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "                                    'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
    "                                    'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
    "                                    'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30849e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created\n",
      "File created\n",
      "File created\n",
      "File created\n",
      "File created\n",
      "File created\n",
      "File created\n",
      "File created\n",
      "File created\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "\n",
    "output_data = pd.DataFrame(columns=['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "                                    'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
    "                                    'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
    "                                    'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])\n",
    "\n",
    "for url, url_id in zip(urls, url_ids):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title_element = soup.find('title')\n",
    "        article_text_element = soup.find('div', {'class': 'td-pb-span8 td-main-content'})\n",
    "\n",
    "        title = title_element.get_text() if title_element else \"Title not found\"\n",
    "        article_text = article_text_element.get_text() if article_text_element else \"Article text not found\"\n",
    "\n",
    "        filename = f\"{url_id}.txt\"\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"{title}\\n\\n{article_text}\")\n",
    "\n",
    "        print(\"File created\")\n",
    "\n",
    "        positive_score, subjectivity_score = calculate_sentiment(article_text)\n",
    "        negative_score = 1 - positive_score\n",
    "        avg_sentence_length, avg_words_per_sentence = calculate_sentence_stats(article_text)\n",
    "        percentage_complex_words, complex_word_count = calculate_complex_word_stats(article_text, complex_words)\n",
    "        fog_index = calculate_fog_index(avg_sentence_length, percentage_complex_words)\n",
    "        syllables_per_word = calculate_syllables_per_word(article_text)\n",
    "        personal_pronoun_count = count_personal_pronouns(article_text, personal_pronouns)\n",
    "        avg_word_length = calculate_avg_word_length(article_text)\n",
    "\n",
    "        output_data_row = pd.DataFrame({\n",
    "            'URL_ID': [url_id],\n",
    "            'URL': [url],\n",
    "            'POSITIVE SCORE': [positive_score],\n",
    "            'NEGATIVE SCORE': [negative_score],\n",
    "            'POLARITY SCORE': [positive_score - negative_score],\n",
    "            'SUBJECTIVITY SCORE': [subjectivity_score],\n",
    "            'AVG SENTENCE LENGTH': [avg_sentence_length],\n",
    "            'PERCENTAGE OF COMPLEX WORDS': [percentage_complex_words],\n",
    "            'FOG INDEX': [fog_index],\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': [avg_words_per_sentence],\n",
    "            'COMPLEX WORD COUNT': [complex_word_count],\n",
    "            'WORD COUNT': [len(nltk.word_tokenize(article_text))],\n",
    "            'SYLLABLE PER WORD': [syllables_per_word],\n",
    "            'PERSONAL PRONOUNS': [personal_pronoun_count],\n",
    "            'AVG WORD LENGTH': [avg_word_length]\n",
    "        })\n",
    "\n",
    "        output_data = pd.concat([output_data, output_data_row], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "\n",
    "output_data.to_excel('output_data.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
